{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kfp --upgrade\n",
    "#!which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Components for Kubeflow Pipelines - script mode\n",
    "In this example we'll build a Kubeflow pipeline using the SageMaker components. Every component calls a different SageMaker feature to perform the following steps:\n",
    "\n",
    "1. Hyperparameter optimization \n",
    "1. Select best hyperparameters and increase epochs\n",
    "1. Training model on the best hyperparameters \n",
    "1. Create an Amazon SageMaker model\n",
    "1. Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp import dsl\n",
    "import time, os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full list of supported components can be found below, along with additional information like runtime arguments for each component:\n",
    "https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker      \n",
    "Now we load the components that for this example, which creates a task factory function. We use this function in our pipeline definition to define the pipeline tasks i.e. container operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_hpo_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/hyperparameter_tuning/component.yaml')\n",
    "sagemaker_train_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/train/component.yaml')\n",
    "sagemaker_model_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/model/component.yaml')\n",
    "sagemaker_deploy_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2/components/aws/sagemaker/deploy/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm   = sess.client('sagemaker')\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training datasets and upload to Amazon S3\n",
    "We are using `generate_cifar10_tfrecords.py` to generate training, test and evaluation datasets and upload these to the Sagemaker default bucket. We recommend to first run the pipeline and then dive into the code, while the pipeline is executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From generate_cifar10_tfrecords.py:35: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From generate_cifar10_tfrecords.py:35: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\n",
      "Download from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and extract.\n",
      "Successfully downloaded cifar-10-python.tar.gz 170498071 bytes.\n",
      "Generating cifar10/train/train.tfrecords\n",
      "Generating cifar10/validation/validation.tfrecords\n",
      "Generating cifar10/eval/eval.tfrecords\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "job_folder      = 'jobs'\n",
    "dataset_folder  = 'datasets'\n",
    "local_dataset = 'cifar10'\n",
    "\n",
    "!python generate_cifar10_tfrecords.py --data-dir {local_dataset}\n",
    "datasets = sagemaker_session.upload_data(path='cifar10', key_prefix='datasets/cifar10-dataset')\n",
    "\n",
    "# If dataset is already in S3 use the dataset's path:\n",
    "# datasets = 's3://{bucket_name}/{dataset_folder}/cifar10-dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload training scripts to Amazon S3\n",
    "We package our training and serving code and upload it to S3. We will pass the `source_s3` parameter to the HPO Tuner and Training Job in our pipeline definition. At the end of the training job the inference code is packaged together with the resulting model stored on S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./requirements.txt\n",
      "./model_def.py\n",
      "./cifar10-training-sagemaker.py\n",
      "./inference.py\n",
      "\n",
      "Uploaded to S3 location:\n",
      "s3://sagemaker-eu-west-1-487575676995/training-scripts/sourcedir.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz sourcedir.tar.gz --exclude=\".ipynb*\" -C code .\n",
    "source_s3 = sagemaker_session.upload_data(path='sourcedir.tar.gz', key_prefix='training-scripts')\n",
    "print('\\nUploaded to S3 location:')\n",
    "print(source_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a custom pipeline op\n",
    "We convert a python function to a component using `func_to_container_op` method, which returns a task factory fucntion similar to loading components from URL. The function will run in a basic python container. The operator takes the best parameters from a hyperparameter tuning job and increases the number of epochs for the next training job. We still keep the number of epochs relatively low as to reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_best_model_hyperparams(hpo_results, best_model_epoch = \"20\") -> str:\n",
    "    import json\n",
    "    r = json.loads(str(hpo_results))\n",
    "    return json.dumps(dict(r,epochs=best_model_epoch))\n",
    "\n",
    "get_best_hyp_op = func_to_container_op(update_best_model_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the pipeline definition. You will have to set the argument for the `role_arn` and change the arguments for `train_image` and `serving_image` at pipeline execution in Kubeflow. You can also set correct default values below. The images will have to be changed to the corresponding ECR registries in Frankfurt.  See the available deep learning container images here: \n",
    "https://github.com/aws/deep-learning-containers/blob/master/available_images.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='cifar10 hpo train deploy pipeline',\n",
    "    description='cifar10 hpo train deploy pipeline using sagemaker'\n",
    ")\n",
    "def cifar10_hpo_train_deploy(region='eu-central-1',\n",
    "                             role_arn='',\n",
    "                           training_input_mode='File',\n",
    "                           train_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:1.15.2-gpu-py36-cu100-ubuntu18.04',\n",
    "                           serving_image='763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:1.15.2-cpu',\n",
    "                           volume_size='50',\n",
    "                           max_run_time='86400',\n",
    "                           instance_type='ml.p3.2xlarge',\n",
    "                           network_isolation='False',\n",
    "                           traffic_encryption='False',\n",
    "                           spot_instance='False',\n",
    "                           channels='[ \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"train\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"'+datasets+'/train\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    }, \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"validation\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"'+datasets+'/validation\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    }, \\\n",
    "                    { \\\n",
    "                        \"ChannelName\": \"eval\", \\\n",
    "                        \"DataSource\": { \\\n",
    "                            \"S3DataSource\": { \\\n",
    "                                \"S3DataType\": \"S3Prefix\", \\\n",
    "                                \"S3Uri\": \"'+datasets+'/eval\", \\\n",
    "                                \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                            } \\\n",
    "                        }, \\\n",
    "                        \"CompressionType\": \"None\", \\\n",
    "                        \"RecordWrapperType\": \"None\" \\\n",
    "                    } \\\n",
    "                ]'\n",
    "                          ):\n",
    "    # Component 1\n",
    "    hpo = sagemaker_hpo_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        strategy='Bayesian',\n",
    "        metric_name='val_acc',\n",
    "        metric_definitions='{\"val_acc\": \"val_acc: ([0-9\\\\\\\\.]+)\"}',\n",
    "        metric_type='Maximize',\n",
    "        static_parameters='{ \\\n",
    "            \"epochs\": \"10\", \\\n",
    "            \"momentum\": \"0.9\", \\\n",
    "            \"weight-decay\": \"0.0002\", \\\n",
    "            \"model_dir\":\"s3://'+bucket_name+'/jobs\", \\\n",
    "            \"sagemaker_program\": \"cifar10-training-sagemaker.py\", \\\n",
    "            \"sagemaker_region\": \"eu-central-1\", \\\n",
    "            \"sagemaker_submit_directory\": \"'+source_s3+'\" \\\n",
    "        }',\n",
    "        continuous_parameters='[ \\\n",
    "            {\"Name\": \"learning-rate\", \"MinValue\": \"0.0001\", \"MaxValue\": \"0.1\", \"ScalingType\": \"Logarithmic\"} \\\n",
    "        ]',\n",
    "        categorical_parameters='[ \\\n",
    "            {\"Name\": \"optimizer\", \"Values\": [\"sgd\", \"adam\"]}, \\\n",
    "            {\"Name\": \"batch-size\", \"Values\": [\"32\", \"128\", \"256\"]}, \\\n",
    "            {\"Name\": \"model-type\", \"Values\": [\"resnet\", \"custom\"]} \\\n",
    "        ]',\n",
    "        channels=channels,\n",
    "        output_location=f's3://{bucket_name}/jobs',\n",
    "        instance_type=instance_type,\n",
    "        instance_count='1',\n",
    "        volume_size=volume_size,\n",
    "        max_num_jobs='4',\n",
    "        max_parallel_jobs='2',\n",
    "        max_run_time=max_run_time,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=spot_instance,\n",
    "        role=role_arn\n",
    "    )\n",
    "    \n",
    "    # Component 2\n",
    "    training_hyp = get_best_hyp_op(hpo.outputs['best_hyperparameters'])\n",
    "    \n",
    "    # Component 3\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        hyperparameters=training_hyp.output,\n",
    "        channels=channels,\n",
    "        instance_type=instance_type,\n",
    "        instance_count='1',\n",
    "        volume_size=volume_size,\n",
    "        max_run_time=max_run_time,\n",
    "        model_artifact_path=f's3://{bucket_name}/jobs',\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=spot_instance,\n",
    "        role=role_arn,\n",
    "    )\n",
    "\n",
    "    # Component 4\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        model_name=training.outputs['job_name'],\n",
    "        image=serving_image,\n",
    "        model_artifact_url=training.outputs['model_artifact_url'],\n",
    "        network_isolation=network_isolation,\n",
    "        role=role_arn\n",
    "    )\n",
    "\n",
    "    # Component 5\n",
    "    prediction = sagemaker_deploy_op(\n",
    "        region=region,\n",
    "        model_name_1=create_model.output,\n",
    "        instance_type_1='ml.m5.large'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the pipeline definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we compile the DSL pipeline definition into a zipped workflow YAML and store it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(cifar10_hpo_train_deploy,'sm-hpo-train-deploy-pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a brief delay you can download the file `sm-hpo-train-deploy-pipeline.zip` from the Jupyter notebook to your machine. Rightclick it and press download. Then go to the Kubeflow dashboard and upload a new Pipeline definition and execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed endpoint\n",
    "The pipeline we ran in Kubeflow deployed the model with the best hyper parameter set to a Sagemaker endpoint. Let's test this endpoint with an example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane 0.0000\n",
      "automobile 0.0000\n",
      "bird 0.0116\n",
      "cat 0.6094\n",
      "deer 0.0010\n",
      "dog 99.1679\n",
      "frog 0.2065\n",
      "horse 0.0035\n",
      "ship 0.0000\n",
      "truck 0.0000\n"
     ]
    }
   ],
   "source": [
    "import json, boto3, numpy as np\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "file_name = '1000_dog.png'\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName='Endpoint-20201207144110-H98G', \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "pred = json.loads(response['Body'].read())['predictions']\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "for l,p in zip(labels, pred[0]):\n",
    "    print(l,\"{:.4f}\".format(p*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
